---
layout: page
title: "Gaussian Process Regression Model in various PPLs"
subtitle: GPR in PPLs
math: on
nburl: "https://github.com/luiarthur/TuringBnpBenchmarks/blob/master/src/gp/notebooks/"
ppls: "Turing,STAN,TFP,Pyro,Numpyro"
sorttable: on
---

<!--
Tables were generated via:
https://jekyllrb.com/tutorials/csv-to-table/
-->


# {{page.title}}

This page was last updated on {{ "now" | date: "%d %b, %Y" }}.

***

In this post, I'll demonstrate how to perform posterior inference for a basic
Gaussian Process (GP) regression model in various probabilistic programming
languages (PPLs). I will first start with a brief overview of GPs and their
utility in modeling functions. Check out the (free) [GP for ML][1] book by
Carl E. Rasmussen for more thorough explanations of GPs. Throughout the post, I
assume an understanding of basic supervised learning methods / techniques (like
linear regression and cross validation), and Bayesian methods.

## Regression

In regression, one learns the relationship between a (possibly multivariate)
response $y_i$ and (one or more) predictors $X_i$. For example, the following
image shows the relationship between noisy univariate responses and predictors
(in dots); the dashed line shows that the true linear relationship response and
predictors.

<img src="{{ "/assets/img/gp/linear-data.png" | prepend: site.baseurl }}"
     class="center" alt="linear data image"/>

A modeler interested in learning meaningful relationships between $y$ and
$X$ (possibly for a new, unobserved $X$) may choose to model this data with a
simple linear regression, in part due to its clear linear trend, as follows:

$$
y_i = f(X_i) + \epsilon_i,
\quad\text{for } i=1,\dots,n
$$

where $f(X_i) = \beta_0 + X_i \cdot \beta_1$ is a linear model, the model
parameters $\beta_0$ and $\beta_1$ are the intercept and slope, respectively;
and $\epsilon_i \sim \text{Normal}(0, \sigma^2)$, with another learnable model
parameter $\sigma^2$.  One way to rewrite this model is

$$
\bm{y} \mid \boldsymbol{\beta}, \sigma^2 \sim
\text{MvNormal}(X \cdot \boldsymbol{\beta}, \sigma^2 \bm{I})
$$

where $\boldsymbol{\beta} = (\beta_0, \beta_1)^T$, MvNormal denotes the
multivariate Normal distribution, and $\bm{I}$ is the identity matrix.
Because of the simplicity of this model and the data, learning the model
parameters is relatively easy.

The following image shows data (dots) where the response $y$ and predictor $x$
have no clear relationship. The dashed line shows the true relationship between
$x$ and $y$, which can be viewed as a smooth (though non-trivial) function. A
linear regression will surely under fit in this scenario. 

<img src="{{ "/assets/img/gp/gp-data-noisy.png" | prepend: site.baseurl }}"
     class="center" alt="GP data image"/>

One of many ways to model this kind of data is via a Gaussian process (GP),
which directly models all the underlying function (in the function space).

> A Gaussian process is a collection of random variables, any Gaussian process
> finite number of which have a joint Gaussian distribution. 
>
> -- [Gaussian Process for Machine Learning][2]

A GP is specified by a mean and covariance function ($m(\bm{x})$ and
$k(\bm{x}, \bm{x}^T)$), so that

$$
\begin{aligned}
f(\bm{x}) &\sim \text{GP}(m(\bm{x}), k(\bm{x}, \bm{x}^T)) \\
m(\bm{x}) &= \text{E}(f(\bm{x})) \\
k(\bm{x}, \bm{x}^T) &=
\text{E}((f(\bm{x}) - m(\bm{x}))(f(\bm{x}^T) - m(\bm{x}^T))). \\
\end{aligned}
$$

Note that the **function** $f(\bm{x})$ is distributed as a GP. For
simplicity (and for certain nice theoretical reasons), in many cases, constant
mean functions ($m(\bm{x})$ equals a constant) and covariance functions
that depend only on the distance $d$ between observations $\bm{x}$ and
$\bm{x}^T$ are used. When this is the case, another way to express a GP
model is 

$$
\bm{y} = f(\bm{X}) \sim \text{MvNormal}(m \cdot \mathbb{1}_N, K)
$$

where $\bm{y}$ is a vector of length $N$ which contains the observed
responses; $\bm{X}$ is an $N\times P$ matrix of covariates; $m$ is a scalar
constant; $\mathbb{1}\_N$ is a vector of ones of length $N$; and $K$ is an
$N\times N$ matrix such that $K_{i,j} = k(\bm{x}\_i, \bm{x}\_j)$.  The
covariance function $k(\bm{x}, \bm{x}^T)$ typically take on special
forms such that $K$ is a valid covariance matrix. Moreover, the covariance
function is often controlled by additional parameters which may control aspects
such as function smoothness and influence of data points on other data points.
For example, a common choice for the covariance function is the squared
exponential covariance function

$$
k(\bm{x}_i, \bm{x}_j) =
\alpha^2 \exp\bc{-\norm{\bm{x}_i - \bm{x}_j}^2_2 / (2\rho^2)}
$$

which is parameterized by a covariance amplitude ($\alpha$) and range parameter
($\rho$). The range controls the correlation between two data points; larger
$\rho$, higher correlation for two data points for a fixed distance. Typically,
it is difficult to exactly specify these parameters, so they are estimated. 
A natural way to do this is via a Bayesian framework, where prior information 
about those parameters can be injected into the model.

A fully Bayesian model for the nonlinear data above can be specified as
follows:

$$
\begin{aligned}
\bm{y} \mid \bm{f} &\sim
\text{MvNormal}(\bm{f}, \sigma^2\bm{I}) \\
\bm{f} \mid \alpha, \rho &\sim
\text{MvNormal}(\bm{0}_N, K_{\alpha,\rho}) \\
\alpha &\sim \text{LogNormal}(0, 0.1) \\
\rho &\sim \text{LogNormal}(0, 1) \\
\sigma &\sim \text{LogNormal}(0, 1)
\end{aligned}
$$

Note that here, I've fixed the mean function to be 0, because the data is
somewhat centered there; but a constant mean function could be learned as well.
A Gaussian likelihood is used here, are observation-level noise is modeled
through $\sigma$. This enables one to marginalize over $\bm{f}$ analytically,
so that:

$$
\begin{aligned}
\bm{y} \mid \alpha, \rho &\sim
\text{MvNormal}(\bm{0}_N, K_{\alpha,\rho} + \sigma^2\bm{I}) \\
\alpha &\sim \text{LogNormal}(0, 0.1) \\
\rho &\sim \text{LogNormal}(0, 1) \\
\sigma &\sim \text{LogNormal}(0, 1)
\end{aligned}
$$

Note that for data with a different support, a different likelihood can be
used; however, the latent function $\bm{f}$ cannot in general be
analytically marginalized out.

The priors chosen need to be be somewhat
informative as there is not much information about the parameters from the data
alone. Here, the prior for $\alpha$ especially favors values near 1, which
means the GP variance will have an amplitude of around 1.

This model can be fit in the probabilistic programming languages (PPLs) 
Turing, Stan, Pyro, Numpyro, and TFP via HMC, NUTS, and ADVI as follows:

<!-- Buttons Div for appending buttons-->
<div id="ppl-buttons" class="btn-group" role="group" aria-label="...">
</div>

{% assign ppl_array = page.ppls | split: ',' %}

{% for ppl in ppl_array %}
  {% assign ppl_lower = ppl | downcase %}

<div class="ppl-code hide" id="{{ppl_lower}}">
<p>
  <a href="{{page.nburl}}/gp_{{ppl_lower}}.ipynb">Full {{ppl}} Example (notebook)</a>
</p>
    {% if ppl_lower == "turing" %}
      {% highlight julia linenos %}{% include_relative ppl-snippets/gp/gp_{{ppl_lower}}.jl %}{% endhighlight %}
    {% else  %}
      {% highlight python linenos %}{% include_relative ppl-snippets/gp/gp_{{ppl_lower}}.py %}{% endhighlight %}
    {% endif %}
</div>
{% endfor %}

## Posterior distributions
The parameters of interest here are the range, amplitude, and model standard
deviation parameters. The image below shows the posterior summaries of these
parameters obtained via NUTS in Turing.  Posterior inference for the parameters
were similar across the different PPLs and inference algorithms. One is
typically interested in estimating the function $f$ over a (fine) grid of input
values, so included is the posterior distribution of $f$ (over a fine grid).
The shaded region is the 95% credible interval, the blue line is the posterior
mean function. The dashed line and dots are the posterior mean of the function
and data, respectively. Note how the credible interval for $f$ is narrower near
where data are observed, and wider when no data are nearby; that is,
uncertainty about the function is greater when you predict the output for
inputs about which you have less information.

<img src="{{ "/assets/img/gp/gp-turing-nuts-post.png" | prepend: site.baseurl }}"
     class="center" alt="Posterior distribution GP turing ADVIimage"/>

## Timings
The table below shows the compile and inference times (seconds) for each of the
PPLs and inference algorithms. (Smaller is better.) Note that the columns can
be sorted by clicking the column headers. The times shown here are the minimum
times from three runs for each algorithm and PPL.

<table class="table table-bordered table-hover table-condensed sortable" id="gp-ppl-times">
  {% for row in site.data.timings.gp.gp_ppl_timings %}
    {% if forloop.first %}
    <tr>
      {% for pair in row %}
        <th>{{ pair[0] }}</th>
      {% endfor %}
    </tr>
    {% endif %}

    {% tablerow pair in row %}
      {{ pair[1] }}
    {% endtablerow %}
  {% endfor %}
</table>

Here are some details for the inference algorithm settings used:
- ADVI
    - Number of ELBO samples was set to 1
    - Number of iterations was set to 2000
- HMC
    - Step size = 0.01
    - Number of leapfrog steps = 100
    - Number of burn-in iterations = 1000
    - Number of subsequent samples collected = 1000
- NUTS
    - Maximum tree depth = 10
    - Target acceptance rate = 80%
    - Adaptation / burn-in period = 1000 iterations
    - Number of sampler collected = 1000

The inference times for all algorithms are lowest in STAN.  Pyro has the
largest inference times for HMC/NUTS (ADVI was not implemented).  All
computations were done in a [c5.xlarge][3] AWS instance. 

<!--TODO
Turing was the only PPL that didn't use all 4 cores. Can I turn it on with
larger dataset?
-->

<!-- Scripts code chunk buttons -->
<script>
$(document).ready(function(){
  // PPLs to benchmark.
  var ppls = ['Turing', 'STAN', 'TFP', 'Pyro', 'Numpyro'];

  for (ppl of ppls) {
    let ppl_lower = ppl.toLowerCase();

    // Create buttons.
    $('#ppl-buttons').append(`
      <button type="button" class="btn btn-outline-primary ${ppl_lower} hide">
        ${ppl}
      </button>
    `);

    // Show Turing example by default.
    $("#turing").removeClass("hide");

    // Button callbacks. 
    $(`button.${ppl_lower}`).click(() => {
      $(".ppl-code").addClass("hide");
      $(`#${ppl_lower}`).removeClass("hide");
    });
  }
});
</script>

[1]: http://www.gaussianprocess.org/gpml/
[2]: http://www.gaussianprocess.org/gpml/chapters/RW2.pdf
[3]: https://aws.amazon.com/ec2/instance-types/c5/
