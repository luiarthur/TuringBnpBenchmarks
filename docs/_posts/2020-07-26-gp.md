---
layout: page
title: "Gaussian Process Regression Model in various PPLs"
subtitle: GPR in PPLs
math: on
nburl: "https://github.com/luiarthur/TuringBnpBenchmarks/blob/master/src/gp/notebooks/"
ppls: "Turing,STAN,TFP,Pyro,Numpyro,NIMBLE"
sorttable: on
date_last_modified: "18 July, 2020."
---

<!--
Tables were generated from csv's at:
https://www.convertcsv.com/csv-to-html.htm
https://jekyllrb.com/tutorials/csv-to-table/
-->


# {{page.title}}

Last updated: {{ page.date_last_modified }}

***

In this post, I'll demonstrate how to perform posterior inference for a basic
Gaussian Process (GP) regression model in various probabilistic programming
languages (PPLs). I will first start with a brief overview of GPs and their
utility in learning from functional data. Check out the (free) [GP for ML][1]
book by Carl E. Rasmussen for more thorough explanations of GPs. Throughout the
post, I assume an understanding of basic supervised learning methods /
techniques (like linear regression and cross validation), and Bayesian methods.

## Functional Data

Functional data describe the relationship between a (possible multivariate)
response $y_i$ and (one or more) predictors $X_i$. For example, the following
image shows the relationship between noisy univariate responses and predictors
(in dots); the dashed line shows that the response and predictors are linearly
related. 

<img src="{{ "/assets/img/gp/linear-data.png" | prepend: site.baseurl }}"
     class="center" alt="linear data image"/>

A modeler interested in learning meaning relationships between $y$ and
$X$ (possibly for a new, unobserved $X$) may choose to model this data with a
simple linear regression

$$
y_i = \beta_0 + X_i \cdot \beta_1 + \epsilon_i,
\quad\text{for } i=1,\dots,n
$$

where the model parameters $\beta_0$ and $\beta_1$ are the intercept and slope,
respectively; and $\epsilon_i \sim \text{Normal}(0, \sigma^2)$, with another
learnable model parameter $\sigma^2$. One way to rewrite this model is

$$
\mathbf{y} \mid \boldsymbol{\beta}, \sigma^2 \sim
\text{MvNormal}(X \cdot \boldsymbol{\beta}, \sigma^2 \mathbf{I})
$$

where $\boldsymbol{\beta} = (\beta_0, \beta_1)^T$, MvNormal denotes the
multivariate Normal distribution, and $\mathbf{I}$ is the identity matrix.
Because of the simplicity of this model and the data, learning the model
parameters is relatively easy.

<img src="{{ "/assets/img/gp/gp-data.png" | prepend: site.baseurl }}"
     class="center" alt="GP data image"/>


## Timings

<table>
  {% for row in site.data.timings.gp.gp_ppl_timings %}
    {% if forloop.first %}
    {% else %}
		<tr>
    <th> {{ row["advi_run"] }} </th>
    <th> {{ row["hmc_run"] }} </th>
    <th> {{ row["nuts_run"] }} </th>
		</tr>
    {% endif %}
  {% endfor %}
</table>

[1]: http://www.gaussianprocess.org/gpml/
