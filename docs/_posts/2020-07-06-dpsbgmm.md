---
layout: page
title: "Dirichlet Process Gaussian mixture model (stick-breaking construction) in various PPLs"
subtitle: DP GMM SB
inNav: on
math: on
---

# {{page.title}}

In this post, I'll explore fitting Dirichlet process Gaussian mixture models
via the stick-breaking construction in various probabilistic programming
languages.

Gaussian mixture models (GMMs) are typically used to (1) approximate an
arbitrary distribution flexibly and/or (2) partition univariate real data
points into groups (clusters). A typical way to specify a GMM is as follows:

$$
\begin{eqnarray}
% Sampling Distribution
y_i \mid z_i = k, \mathbf{w}, \mathbf{\mu}, \mathbf{\sigma^2} &\sim& 
\text{Normal}(\mu_k, \sigma^2_k), & \text{for } i=1,\dots,N \\
% Priors
z_i \mid \mathbf{w} &\sim& \text{Categorical}(w), & \text{for } i = 1,\dots,N \\
\mu_k &\sim& \text{Normal}(m, s^2), & \text{for } k=1,\dots,K \\
\sigma^2_k &\sim& \text{InverseGamma}(a, b), & \text{for } k=1,\dots,K \\
\mathbf{w} &\sim& \text{Dirichlet}_K(1/K) \\
\end{eqnarray}
$$

where (1) is the sampling distribution (or model) with $N$ observations $y_i$
for $i=1,\dots,N$, and (2) - (5) are priors for the parameters $\mathbf{z}$ (an
integer vector of cluster memberships), $\mathbf{\mu}$ (a real $K$-dimensional
vector of mixture locations), $\mathbf{\sigma^2}$ (a $K$-dimensional vector of
mixture scales, with each element being positive and real), and a
$K$-dimensional probability vector $\mathbf{w}$ (with all non-negative
elements, and which sums to 1). Alternatively, a probability vector is often
referred to as a simplex.

With a model and priors on model parameters, we have a fully specified Bayesian
model. Inference can be made on model parameters to learn about the data
$\mathbf{y}$. More specifically, given prior information on model parameters,
and an appropriate model, we can make posterior inference on model parameters.
Many posterior inference algorithms can be used, 

Note that (posterior distributions of) clusterings of (or a partition on) the
data $\mathbf{y}$ can be generated from this model via $\mathbf{z}$.

A challenge in applying GMMs is determining the number of clusters ($K$).  The
prior used for $\mathbf{w}$ in this model encourages sparsity; i.e. some
elements in $\mathbf{w}$ are encouraged to be large, while others are encourage
to be close to 0. Thus, one may use a sufficiently large (but practical) $K$,
and allow the model to learn a sparse $\mathbf{w}$. Counting only the number of
weights in $\mathbf{w}$ that are noticeably above 0 (*a posteriori*) will
yield posterior estimates of the number of clusters.

<!-- DP -->
<!-- Another class of sparse priors for -->
