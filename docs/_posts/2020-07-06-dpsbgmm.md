---
layout: page
title: "Dirichlet Process Gaussian mixture model (stick-breaking construction) in various PPLs"
subtitle: DP GMM SB
inNav: on
math: on
---

# {{page.title}}

In this post, I'll explore fitting Dirichlet process Gaussian mixture models
via the stick-breaking construction in various probabilistic programming
languages.

Gaussian mixture models (GMMs) are typically used to (1) approximate an
arbitrary distribution flexibly and/or (2) partition univariate real data
points into groups (clusters). A GMM can be specified as follows:

$$
\begin{eqnarray}
% Sampling Distribution
y_i \mid \mathbf{w}, \mathbf{\mu}, \mathbf{\sigma^2} &\sim& 
\sum_{k=1}^K \text{Normal}(\mu_k, \sigma^2_k), & \text{for } i=1,\dots,N \\
% Priors
\mu_k &\sim& p(\mu_k) & \text{for } k=1,\dots,K \\
\sigma^2_k &\sim& p(\sigma^2_k) & \text{for } k=1,\dots,K \\
\mathbf{w} &\sim& p(\mathbf{w}) \\
\end{eqnarray}
$$

where (1) is the sampling distribution (or model) with $N$ observations $y_i$
for $i=1,\dots,N$, and (2) - (4) are priors for the parameters $\mathbf{\mu}$
(a real $K$-dimensional vector of mixture locations) , $\mathbf{\sigma^2}$ (a
$K$-dimensional vector of mixture scales, with each element being positive and
real), and a $K$-dimensional probability vector $\mathbf{w}$ (with all
non-negative elements, and which sums to 1). Alternatively, a probability
vector is often referred to as a simplex.

With a model and priors on model parameters, we have a fully specified Bayesian
model. Inference can be made on model parameters to learn about the data
$\mathbf{y}$. More specifically, given prior information on model parameters,
and an appropriate model, we can make posterior inference on model parameters.
Many posterior inference algorithms can be used, 

Clusterings of (or a partition on) the data $\mathbf{y}$ can be generated from
this model in at least two ways. First, 

A challenge in applying GMMs is determining the 
