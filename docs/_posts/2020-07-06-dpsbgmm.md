---
layout: page
title: "Dirichlet Process Gaussian mixture model via the stick-breaking construction in various PPLs"
subtitle: DP GMM SB
math: on
nburl: "https://github.com/luiarthur/TuringBnpBenchmarks/blob/master/src/dp-gmm/notebooks/"
ppls: "Turing,STAN,TFP,Pyro,Numpyro,NIMBLE"
sorttable: on
date_last_modified: "10 August, 2020."
---

<!--
Tables were generated from csv's at:
https://www.convertcsv.com/csv-to-html.htm
-->

# {{page.title}}

Last updated: {{ page.date_last_modified }}

***

In this post, I'll explore implementing posterior inference for Dirichlet
process Gaussian mixture models (GMMs) via the stick-breaking construction in
various probabilistic programming languages. For an overview of the Dirichlet
process (DP) and Chinese restaurant process (CRP), visit this post on
[Probabilistic Modeling using the Infinite Mixture Model][1] by the [Turing
team][11]. Basic familiarity with Gaussian mixture models and Bayesian methods
are assumed in this post. This [Coursera Course on Mixture Models][2] offers a
great intro on the subject.

## Stick-breaking Model for Mixture Weights

As described in [Probabilistic Modeling using the Infinite Mixture
Model][1], an Bayesian infinite GMM can be expressed as follows

$$
\begin{eqnarray}
  \alpha &\sim& \t{Gamma(a, b)} &\t{(optional prior)} \\
  \mathbf{z} \mid \alpha &\sim& \t{CRP}_N(\alpha) \\
  (\mu_k, \sigma_k) &\sim& G_0, &\t{for } k=1, \dots, \infty \\
  y_i \mid z_i, \mathbf{\mu}, \mathbf{\sigma} &\sim& \t{Normal}(\mu_{z_i}, \sigma_{z_i}),
  & \t{for } i=1, \dots, N \\
\end{eqnarray}
$$

Note that this is a collapsed version of a DP (Gaussian) mixture model.
Equation (4) is the sampling distribution or model, where each of $N$ real
univariate observations ($y_i$) are assumed to be Normally distributed; (3) are
independent priors for the mixture locations and scales. For example, the
distribution $G$ could be $\t{Normal}(\cdot \mid m, s) \times
\t{Gamma}(\cdot \mid a, b)$; i.e. a bivariate distribution with independent
components -- one Normal and one Gamma. The CRP allows for the number of
mixture components to be unbounded (though it is practically bounded by $N$),
hence $k=1,\dots,\infty$. (2) is the prior for the cluster membership
indicators. Through $z_i$, the data can be partitioned (i.e. grouped) into
clusters; (1) is an optional prior for the concentration parameter $\alpha$
which influences the number of clusters; larger $\alpha$, greater number of
clusters. Note that the data usually does not contain much information about
$\alpha$, so priors for $\alpha$ need to be at least moderately informative.

This model is difficult to implement generically in PPLs because it (1) allows
the number of mixture components (i.e. the dimensions of $\mu$ and $\sigma$) to
vary, and (2) it contains discrete parameters. It is true that for a particular
class of DP mixture models (for carefully chosen priors), efficient posterior
inference is just a matter of iterative and direct sampling from full
conditionals. But this convenience is not available in general, and we still
have the issue of varying number of mixture components. The PPL NIMBLE (in R)
addresses this issue by allowing the user to specify a maximum number of
components, it also cleverly exploits conjugate priors when possible. This
feature is not typically seen in PPLs, including Turing (at the moment). NIMBLE
also supports the sampling of discrete parameters, which is uncommon in PPLs
which usually implement generic and efficient sampling algorithms like HMC and
NUTS for continuous parameters. Turing supports the sampling of discrete
parameters via sequential Monte Carlo (SMC) and particle Gibbs (PG) /
conditional SMC.

Having said that, the **finite** stick-breaking construction of the DP bypasses
the need for varying number of mixture components. In the context of mixture
models, the **infinite** stick-breaking construction of the DP constructs
mixture weights $(w_1, w_2, \dots)$ as follows:

$$
\begin{eqnarray*}
  v_\ell \mid \alpha &\sim& \t{Beta}(1, \alpha), &\t{for } \ell\in\mathbb{N} \\
  w_1 &=& v_1 \\
  w_k &=& v_k \prod_{\ell=2}^{k - 1} 1 - v_\ell, &\t{for } k\in\bc{2,3,\dots} \\
\end{eqnarray*}
$$

This image shows a realization of $\mathbf{w}$ from the stick-breaking process
with $\alpha=3$.

<img src="{{ "/assets/img/dp-sb-gmm/sb.png" | prepend: site.baseurl }}"
     class="center" alt="stickbreak image"/>

Infinite probability vectors ($\mathbf{w}$) generated from this construction
are equivalent to the weights implied by the CRP (with parameter $\alpha$).
Hence, the probability vector $\mathbf{w}$ (aka simplex), where each element is
non-negative and the elements sum to 1, will be sparse. That is, most of the
weights will be extremely close to 0, and only a handful will be noticeably (to
substantially) greater than 0. The sparsity will be influenced by $\alpha$. In
practice, a truncated (or finite) version of the stick-breaking
construction is used. (An infinite array cannot be created on a computer...)
The finite stick-breaking model simply places an upper bound on the number of
mixture components, which, if chosen reasonably, allows us to reap the benefits
of the DP (a model which allows model complexity to grow with data size) at a
cheaper computational cost. The finite version is as follows:

$$
\begin{eqnarray}
  v_\ell \mid \alpha &\sim& \t{Beta}(1, \alpha), &\t{for } \ell=1,\dots,K-1 
  \nonumber \\
  w_1 &=& v_1 \label{eq:sb-start} \\
  w_k &=& v_k \prod_{\ell=2}^{k - 1} 1 - v_\ell, &\t{for } k=2,\dots,K-1 \\
  w_K &=& \prod_{\ell=1}^{K - 1} 1 - v_\ell \label{eq:sb-end}.
\end{eqnarray}
$$

Note that $w_K$ is defined such that $w_K = 1 - \sum_{k=1}^{K-1} w_k$.  But
$(\ref{eq:sb-end})$ is typically implemented in software for numerical
stability.  Here on, for brevity, $\mathbf{w} = \t{stickbreak}(\mathbf{v})$
will denote the transformation in $(\ref{eq:sb-start})$ - $(\ref{eq:sb-end})$.
Notice that the probability vector $\mathbf{w}$ is $K$-dimensional, while $v$
is $(K-1)$-dimensional. (A simplex $\mathbf{w}$ of length $K$ only requires
$K-1$ elements to be specified. The remaining element is constrained such that
the probability vector sums to 1.)

A DP GMM under the stick-breaking construction can thus be specified as
follows:

$$
\begin{eqnarray}
  % Priors.
  \alpha &\sim& \t{Gamma(a, b)} &\t{(optional prior)} \\
  v_k \mid \alpha &\sim& \t{Beta}(1, \alpha), &\t{for } k=1,\dots,K-1 \\
  \mathbf{w} &=& \t{stickbreak}(\mathbf{v}), \\
  \mathbf{z} \mid \mathbf{w} &\sim& \t{Categorical}_K(\mathbf{w}) \\
  \mu_k &\sim& G_\mu, &\t{for } k = 1,\dots,K \\
  \sigma_k &\sim& G_\sigma, &\t{for } k = 1,\dots,K \\
  
  % Sampling Distribution.
  y_i \mid z_i, \mathbf{\mu}, \mathbf{\sigma} &\sim& \t{Normal}(\mu_{z_i}, \sigma_{z_i}),
  & \t{for } i=1, \dots, N \\
\end{eqnarray}
$$

where $G_\mu$ and $G_\sigma$ are appropriate priors for $\mu_k$ and $\sigma_k$,
respectively.  Marginalizing over the (discrete) cluster membership indicators
$\mathbf{z}$ may be beneficial in practice if an efficient posterior inference
algorithm (e.g.  [ADVI][3], [HMC][4], [NUTS][5]) exists for learning the joint
posterior of the remaining model parameters. If this is the case, one further
reduction can be made to yield:

$$
\begin{eqnarray}
  % Priors.
  \alpha &\sim& \t{Gamma(a, b)} &\t{(optional prior)}. \\
  v_k \mid \alpha &\sim& \t{Beta}(1, \alpha), &\t{for } k=1,\dots,K-1 \\
  \mathbf{w} &=& \t{stickbreak}(\mathbf{v}) \\

  % Sampling Distribution.
  \mu_k &\sim& G_\mu, &\t{for } k = 1,\dots,K \\
  \sigma_k &\sim& G_\sigma, &\t{for } k = 1,\dots,K \\
  y_i \mid \mathbf{\mu}, \mathbf{\sigma}, \mathbf{w} &\sim&
  \sum_{k=1}^K w_k \cdot \t{Normal}(\mu_k, \sigma_k),
  & \t{for } i=1, \dots, N \\
\end{eqnarray}
$$

The joint posterior of the parameters $\mathbf{\theta} = \p{\mathbf{\mu},
\mathbf{\sigma}, \mathbf{v}, \alpha}$ can be sampled from using NUTS or HMC, or
approximated via ADVI. This can be done in the various PPLs as follows. (Note that
these are excerpts from complete examples which are also linked.) 

<!-- Buttons Div for appending buttons-->
<div id="ppl-buttons" class="btn-group" role="group" aria-label="...">
</div>

{% assign ppl_array = page.ppls | split: ',' %}

{% for ppl in ppl_array %}
  {% assign ppl_lower = ppl | downcase %}

  {% if ppl_lower == "nimble" %}
<div class="ppl-code hide" id="{{ppl_lower}}">
  <p>
    ADVI, HMC, and NUTS are not supported in NIMBLE at the moment. Though,
    the model can be implemented and posterior inference can be made via
    alternative inference algorithms. See:
    <a href="https://r-nimble.org/html_manual/cha-bnp.html#sec:sb">
      this NIMBLE example.
    </a>
  </p>
</div>
  {% else %}
<div class="ppl-code hide" id="{{ppl_lower}}">
<p>
  <a href="{{page.nburl}}/dp_sb_gmm_{{ppl_lower}}.ipynb">Full {{ppl}} Example (notebook)</a>
</p>
    {% if ppl_lower == "turing" %}
      {% highlight julia linenos %}{% include_relative ppl-snippets/dp-sb-gmm/dp_sb_gmm_{{ppl_lower}}.jl %}{% endhighlight %}
    {% elsif ppl_lower == "nimble" %}
      {% highlight R linenos %}{% include_relative ppl-snippets/dp-sb-gmm/dp_sb_gmm_{{ppl_lower}}.R %}{% endhighlight %}
    {% else  %}
      {% highlight python linenos %}{% include_relative ppl-snippets/dp-sb-gmm/dp_sb_gmm_{{ppl_lower}}.py %}{% endhighlight %}
    {% endif %}
</div>
  {% endif %}
{% endfor %}

The purpose of this post is to compare various PPLs for this particular model.
That includes things like timings, inference quality, and syntax sugar.
Comparing inferences is difficult without overwhelming the reader with many
figures. Suffice it to say that , where possible, HMC and NUTS performed
similarly across the various PPLs. And some discrepancies occurred for ADVI.
(Full notebooks with visuals are provided [here][8].) Nevertheless, I have
included some information of the data used, and inferences from ADVI in Turing
here. (The inferences from HMC and NUTS were not vastly different.)

Here is a histogram of the data used. $y_i$ for $i=1,\dots,200$ is univariate
simulated from a mixture of four Normal distributions with varying locations
and scales. The specific dataset can be found [here][9], and the scripts for
generating the data can be found [here][10].

<img src="{{ "/assets/img/dp-sb-gmm/simdata-n200.png" | prepend: site.baseurl }}"
     class="center" alt="simdata-n200" style="" />

Here we have the posterior distributions of $\eta$ (the mixture weights, aka
$w$ above), $\mu$ (the mixture locations), $\sigma$ (the mixture scales),
and $\alpha$ the concentration parameter.

<img src="{{ "/assets/img/dp-sb-gmm/advi-n200.png" | prepend: site.baseurl }}"
     class="center" alt="advi-n200-posterior" />

Note that the whiskers in the box plots are the ends of the 95% credible
intervals. The triangles and solid horizontal line in the boxplots are the
posterior means, and medians, respectively. The dashed lines are the simulation
truths, which match up closely to the posterior means, and fall within the 95%
credible intervals. Note for components 5-10, $\eta$ is near 0, due to the
sparse prior which is implied on it. In addition, $\mu$ and $\sigma$ for
components 5-10 are simply sampling from the prior, as no information is
available for those components (which are not used). The histogram for $\alpha$
is provided for reference. Since there is no simulation truth for $\alpha$, not
much more can be said about it. It is reasonable that it is in the range (0, 1)
due to the sparseness of the small number of mixture components and the prior
information. We can see that this model effectively learned there are only
4 clusters in this data.

## Comparing the PPLs

First off, these are the settings used for HMC, and NUTS:
- ADVI:
    - The optimizers for ADVI were run for 2000 iterations.
    - Full ADVI was done (i.e. no sub-sampling of the data as in stochastic
      ADVI)
    - STAN defaults to using 100 samples for ELBO approximation
      and 10 samples for ELBO gradient approximation. When I set these to 1,
      like in Turing (and also recommended in the ADVI paper), the inferences
      were quite bad, so I left them at the default. Nevertheless, STAN still
      ran the fastest.
    - NOTE: ADVI is extremely sensitive to initial values. The best run from
      multiple runs with varying initial values was used.
    - NOTE: There are discrepancies in the optimizers used in each PPL.
      In Pyro and Turing, the Adam optimizer was used; in STAN, RmsProp was
      used.
    - NOTE: Due to my limited understanding of the documentation in Numpyro,
      ADVI could not be used in those PPLs. Though, the documentation
      indicates that it is possible to use ADVI through the `SVI` class.
- HMC
    - Step-size: 0.01
    - Number of leapfrog steps: 100
    - Number of burn-in iterations: 500
    - Number of subsequent HMC iterations: 500
        - This is also the number of posterior samples
    - NOTE: Relatively robust to initial values.
- NUTS
    - Target acceptance rate: 80%
    - Number of iterations for burn-in: 500
        - NOTE: In TFP, the number of iterations for adapting NUTS
          hyper-parameters was set to 400 (this needs to be less than the
          burn-in, as it is part of the burn-in period).
    - Maximum tree-depth: 10
    - Number of subsequent NUTS iterations: 500
        - This is also the number of posterior samples
    - NOTE: Relatively robust to initial values.
- Note: Automatic differentiation (AD) libraries used vary between the
  PPLs. The default AD libraries in each PPL were used. This may have a 
  small effect on the timings.


Specifying the DP GMM via stick-breaking is quite simple in each of the PPLs.
Each PPL has also strengths and weaknesses for this particular model.
Particularly, some PPLs had shorter compile times; others had short
inference-times. It appears that the inferences from HMC and NUTS are similar
across the PPLs. Pyro implements ADVI, but the inferences were quite poor
compared to STAN, Turing, and TFP (several random initial values were used).
Numpyro was fastest at HMC and NUTS. STAN was fastest at ADVI. Compile times
were shortest for TFP and Pyro.

STAN, being the oldest of the PPLs, has the most comprehensive documentation.
So, implementing this particular model was quite easy.

TFP is quite verbose, but offers a lot of control for the user. I had some
difficulty getting the dimensions (`batch_dim`, `event_shape`) correct
initially. [Dave Moore][12], who works at Google's BayesFlow team, gracefully 
looked at my code and offered fixes!

I would like to point out (while acknowledging my current affiliation with the
Turing team) the elegance of the syntax of Turing's model specification. It is
close to how one would naturally write down the model, and it is also the
shortest. For those that are already familiar with Julia, custom functions
(such as the [`stickbreak`][13] function used) can be implemented by the user
and used in the model specification. Note that functions from other libraries
can be used quite effortlessly. For example, in Turing, the `logsumexp` and
`normlogpdf` methods are from the `StatsFuns.jl` library. They worked without
any tweaking in Turing. Another strength of Turing sampling from the full
conditionals of individual (or a group of) parameters using different inference
algorithms is possible. For example, 
```julia 
hmc_chain = sample(dp_gmm_sb(y, 500),  # data, number of mixture components
                   Gibbs(HMC(0.01, 100, :mu, :sig, :v),
                         MH(:alpha)),
                   1000)  # iterations
```
enables the sampling of $(\mu, \sigma, v)$ jointly conditioned on the current
$\alpha$, using HMC; and sampling $\alpha \mid
\mathbf{\mu},\mathbf{\sigma},\mathbf{v}$ via a vanilla Metropolis-Hastings
step. This is not possible in STAN, Pyro, Numpyro, and TFP. (This is also
possible in NIMBLE; however, NIMBLE currently does not support inference
algorithms based on AD.) Another possibility in Turing is:
```julia 
hmc_chain = sample(dp_gmm_sb(y, 500),  # data, number of mixture components
                   Gibbs(HMC(0.01, 100, :mu, :sig),
                         HMC(0.01, 100, :v),
                         MH(:alpha)),
                   1000)  # iterations
```
where separate HMC samplers are used for $(\mu, \sigma)$ and $\mathbf{v}$.

For HMC, the inference timings of Turing are within an order of magnitude from
the fastest PPL (Numpyro). I think time and inference comparisons via HMC may
be fairer than the NUTS and ADVI comparisons as the implementations of HMC is
comparatively more well-defined (standardized) and relatively simple; whereas
the implementations of ADVI and NUTS are nuanced, and for NUTS quite complex.
As already stated, the quality of AD libraries will affect timings, and
possibly the quality of the inferences. Though the AD libraries used (Flux,
autodiff, torch, tensorflow, JAX) all seem rather robust.

### Timings
<!-- TODO: explain that these are one-off timings. Could use the minimums? -->
Here are the compile and inference times (in seconds) for each PPL for this
model. Smaller is better. (By clicking the column headers, you can sort the
rows by inference times.)

<table class="table table-bordered table-hover table-condensed sortable" id="dpsbgmm-ppl-times">
  {% for row in site.data.timings.dpsbgmm.dpsbgmm_ppl_timings %}
    {% if forloop.first %}
    <tr>
      {% for pair in row %}
        <th>{{ pair[0] }}</th>
      {% endfor %}
    </tr>
    {% endif %}

    {% tablerow pair in row %}
      {{ pair[1] }}
    {% endtablerow %}
  {% endfor %}
</table>

Note that timings for ADVI were listed as "NA" for Numpyro because the
documentation indicates it is possible to use ADVI, but I was not able to
implement it successfully due to my lack of understanding of the documentation.
Numpyro is a very promising PPL, but still relatively young, so syntax may
still unstable and documentation is being updated constantly.

For STAN, the compile times listed is the time required to compile the model;
i.e. the time required to run this command:
```python
sm = pystan.StanModel(model_code=model)
```
The model is compiled only once (and not three times), then the user is free to
select the inference algorithm.

The timings provided are one-off, but they don't vary much from run-to-run, and
don't affect the rankings of the PPLs in terms of speed for each inference
algorithm.
<!-- 
To be more precise, one could run the analyses several times, and report the
minimum timings.
-->

Note that for the Turing model, noticeable (40-60%) speedups can be realized by
replacing the `UnivariateGMM` model by a direct increment of the log
unnormalized joint posterior pdf. See this [implementation][14] and the results
[timings][15].


The `Manifest.toml` and `requirements.txt` files, respectively, in the [GitHub
project page][6] list the specific Julia and Python libraries used for these
runs. All experiments for this project were done in an [c5.xlarge][7] AWS Spot
Instance. As of this writing, here are the specs for this instance:

- vCPU: 4 Intel(R) Xeon(R) Platinum 8124M CPU @ 3.00GHz
- RAM: 8 GB
- Storage: EBS only (32 GB)
- Network Bandwidth: Up to 10 Gbps
- EBS Bandwidth: Up to 4750 Mbps

## Next

<!-- 
In my next post, I will compare inferences from the CRP construction of the DP
for infinite GMMs in Turing and NIMBLE.
-->
In my next post, I will do a similar comparison of the PPLs and inference
algorithms mentioned here for a basic Gaussian process model.

Feel free to comment below.

<!-- Scripts code chunk buttons -->
<script>
$(document).ready(function(){
  // PPLs to benchmark.
  var ppls = ['Turing', 'STAN', 'TFP', 'Pyro', 'Numpyro', 'NIMBLE'];

  for (ppl of ppls) {
    let ppl_lower = ppl.toLowerCase();

    // Create buttons.
    $('#ppl-buttons').append(`
      <button type="button" class="btn btn-default ${ppl_lower}">${ppl}</button>
    `);

    // Show Turing example by default.
    $("#turing").attr("class", `ppl-code show`);

    // Button callbacks. 
    $(`button.${ppl_lower}`).click(() => {
      $(".ppl-code").attr("class", `ppl-code hide`);
      $(`#${ppl_lower}`).attr("class", `ppl-code show`);
    });
  }
});
</script>

[1]: https://turing.ml/dev/tutorials/6-infinitemixturemodel/
[2]: https://www.coursera.org/learn/mixture-models
[3]: https://luiarthur.github.io/statorial/varinf/introvi/
[4]: https://arxiv.org/abs/1206.1901
[5]: http://jmlr.org/papers/v15/hoffman14a.html
[6]: https://github.com/luiarthur/TuringBnpBenchmarks
[7]: https://aws.amazon.com/ec2/instance-types/c5/
[8]: https://github.com/luiarthur/TuringBnpBenchmarks/tree/master/src/dp-gmm/notebooks 
[9]: https://github.com/luiarthur/TuringBnpBenchmarks/blob/master/src/data/sim-data/gmm-data-n200.json
[10]: https://github.com/luiarthur/TuringBnpBenchmarks/blob/master/src/data/generate-data.jl
[11]: https://turing.ml/dev/
[12]: https://davmre.github.io/ 
[13]: https://github.com/TuringLang/Turing.jl/blob/6db59629f1f189f63350aef9ce4fe6c0bebdaba1/src/stdlib/RandomMeasures.jl#L216
[14]: https://github.com/luiarthur/TuringBnpBenchmarks/blob/master/src/dp-gmm/notebooks/dp_sb_gmm_turing_alt.ipynb 
[15]: https://github.com/luiarthur/TuringBnpBenchmarks/blob/master/docs/_data/timings/dpsbgmm/dpsbgmm_ppl_timings_optimized.csv
