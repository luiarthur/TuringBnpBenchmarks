---
layout: page
title: "Dirichlet Process Gaussian mixture model via the stick-breaking construction in various PPLs"
subtitle: DP GMM SB
math: on
nburl: "https://github.com/luiarthur/TuringBnpBenchmarks/blob/master/src/dp-gmm/notebooks/"
ppls: "Turing,STAN,TFP,Pyro,Numpyro,NIMBLE"
---

<style>
  div.ppl-code p {
    font-size: 80%;
    padding: 10px;
  }
</style>

# {{page.title}}

Last updated: 7 July, 2020.

***

In this post, I'll explore fitting Dirichlet process Gaussian mixture models
(GMMs) via the stick-breaking construction in various probabilistic programming
languages. For an overview of the Dirichlet process (DP) and Chinese restaurant
process (CRP), visit this post on [Probabilistic Modeling using the Infinite
Mixture Model][1] by the Turing team. Basic familiarity with Gaussian mixture
models and Bayesian methods are assumed in this post. This [Coursera Course on
Mixture Models][2] offers a great intro on the subject.

## Stick-breaking Model for Mixture Weights

As described in [Probabilistic Modeling using the Infinite Mixture
Model][1], an Bayesian infinite GMM can be expressed as follows

$$
\begin{eqnarray}
  y_i \mid z_i, \mathbf{\mu}, \mathbf{\sigma} &\sim& \t{Normal}(\mu_{z_i}, \sigma_{z_i}),
  & \t{for } i=1, \dots, N \\
  (\mu_k, \sigma_k) &\sim& G_0, &\t{for } k=1, \dots, \infty \\
  \mathbf{z} \mid \alpha &\sim& \t{CRP}_N(\alpha) \\
  \alpha &\sim& \t{Gamma(a, b)} &\t{(optional prior)} \\
\end{eqnarray}
$$

Note that this is a collapsed version of a DP (Gaussian) mixture model.
Equation (1) is the sampling distribution or model, where each of $N$ real
univariate observations ($y_i$) are assumed to be Normally distributed; (2) are
independent priors for the mixture locations and scales. For example, the
distribution $G$ could be $\t{Normal}(\cdot \mid m, s) \times
\t{Gamma}(\cdot \mid a, b)$; i.e. a bivariate distribution with independent
components -- one Normal and one Gamma. The CRP allows for the number of
mixture components to be unbounded (though it is practically bounded by $N$),
hence $k=1,\dots,\infty$. (3) is the prior for the cluster membership
indicators. Through $z_i$, the data can be partitioned (i.e. grouped) into
clusters; (4) is an optional prior for the parameter $\alpha$ which influences
the number of clusters; larger $\alpha$, greater number of clusters. Note that
the data usually does not contain much information about $\alpha$, so priors
for $\alpha$ need to be at least moderately informative.

This model is difficult to implement generically in PPLs because it (1) allows
the number of mixture components (i.e. the dimensions of $\mu$ and $\sigma$) to
vary, and (2) it contains discrete parameters. It is true that for a particular
class of DP mixture models (for carefully chosen priors), efficient posterior
inference is just a matter of iterative and direct sampling from full
conditionals. But this convenience is not available in general, and we still
have the issue of varying number of mixture components. The PPL NIMBLE (in R)
addresses this issue by allowing the user to specify a maximum number of
components, it also cleverly exploits conjugate priors when possible. This
feature is not typically seen in PPLs, including Turing (at the moment). NIMBLE
also supports the sampling of discrete parameters, which is uncommon in PPLs
which usually implement generic and efficient sampling algorithms like HMC and
NUTS for continuous parameters. Turing supports the sampling of discrete
parameters via sequential Monte Carlo (SMC) and particle Gibbs (PG) /
conditional SMC.

Having said that, the **finite** stick-breaking construction of the DP bypasses
the need for varying number of mixture components. In the context of mixture
models, the **infinite** stick-breaking construction of the DP constructs
mixture weights $(w_1, w_2, \dots)$ as follows:

$$
\begin{eqnarray*}
  v_\ell \mid \alpha &\sim& \t{Beta}(1, \alpha), &\t{for } \ell\in\mathbb{N} \\
  w_1 &=& v_1 \\
  w_k &=& v_k \prod_{\ell=2}^{k - 1} 1 - v_\ell, &\t{for } k\in\bc{2,3,\dots} \\
\end{eqnarray*}
$$

Weights ($\mathbf{w}$) generated from this construction are equivalent to the
weights implied by the CRP (with parameter $\alpha$). Hence, the probability
vector $\mathbf{w}$ (aka simplex), where each element is non-negative and the
weights sum to 1, will be sparse. i.e. most of the weights will be extremely
close to 0, and only a handful will be noticeably (to substantially) greater
than 0. The sparsity will be influenced by $\alpha$. For practicality, a
truncated (or finite) version of the stick-breaking construction is used. (An
infinite cannot be created on a computer...) The finite stick-breaking model
simply places an upper bound on the number of mixture components, which, if
chosen reasonably, allows us to reap the benefits of the DP (a model which
allows model complexity to grow with data size) at a cheaper computational
cost. The finite version is as follows:

$$
\begin{eqnarray*}
  v_\ell \mid \alpha &\sim& \t{Beta}(1, \alpha), &\t{for } \ell=1,\dots,K-1 \\
  w_1 &=& v_1 \\
  w_k &=& v_k \prod_{\ell=2}^{k - 1} 1 - v_\ell, &\t{for } k=2,\dots,K-1 \\
  w_K &=& \prod_{\ell=1}^{K - 1} 1 - v_\ell.
\end{eqnarray*}
$$

Notice that the probability vector $\mathbf{w}$ is $K$-dimensional, while $v$
is $(K-1)$-dimensional. ($\mathbf{w}$ of length $K$ only requires $K-1$
elements to be specified. The remaining element must be such that the
probability vector sums to 1.)

A DP GMM under the stick-breaking construction can thus be specified as
follows:

$$
\begin{eqnarray}
  y_i \mid z_i, \mathbf{\mu}, \mathbf{\sigma} &\sim& \t{Normal}(\mu_{z_i}, \sigma_{z_i}),
  & \t{for } i=1, \dots, N \\
  \mathbf{z} \mid \mathbf{w} &\sim& \t{Categorical}_K(\mathbf{w}) \\
  \alpha &\sim& \t{Gamma(a, b)} &\t{(optional prior)} \\
  v_\ell \mid \alpha &\sim& \t{Beta}(1, \alpha), &\t{for } \ell=1,\dots,K-1 \\
  w_1 &=& v_1 \\
  w_k &=& v_k \prod_{\ell=2}^{k - 1} 1 - v_\ell, &\t{for } k=2,\dots,K-1 \\
  w_K &=& \prod_{\ell=1}^{K - 1} 1 - v_\ell.
\end{eqnarray}
$$

More succinctly,

$$
\begin{eqnarray}
  y_i \mid z_i, \mathbf{\mu}, \mathbf{\sigma} &\sim& \t{Normal}(\mu_{z_i}, \sigma_{z_i}),
  & \t{for } i=1, \dots, N \\
  \mu_k &\sim& G_\mu, &\t{for } k = 1,\dots,K \\
  \sigma_k &\sim& G_\sigma, &\t{for } k = 1,\dots,K \\
  \mathbf{z} \mid \mathbf{w} &\sim& \t{Categorical}_K(\mathbf{w}) \\
  \mathbf{w} &=& \t{stickbreak}(\mathbf{v}), \\
  v_\ell \mid \alpha &\sim& \t{Beta}(1, \alpha), &\t{for } \ell=1,\dots,K-1 \\
  \alpha &\sim& \t{Gamma(a, b)} &\t{(optional prior)} \\
\end{eqnarray}
$$

where $G_\mu$ and $G_\sigma$ are appropriate priors for $\mu_k$ and $\sigma_k$,
respectively.  Marginalizing over the (discrete) cluster membership indicators
$\mathbf{z}$ may be beneficial in practice if an efficient posterior inference
algorithm (e.g.  [ADVI][3], [HMC][4], [NUTS][5]) exists for learning the joint
posterior of the remaining model parameters. If this is the case, one further
reduction can be made to yield:

$$
\begin{eqnarray}
  y_i \mid \mathbf{\mu}, \mathbf{\sigma} &\sim&
  \sum_{k=1}^K \t{Normal}(\mu_k, \sigma_k),
  & \t{for } i=1, \dots, N \\
  \mu_k &\sim& G_\mu, &\t{for } k = 1,\dots,K \\
  \sigma_k &\sim& G_\sigma, &\t{for } k = 1,\dots,K \\
  \mathbf{w} &=& \t{stickbreak}(\mathbf{v}) \\
  v_\ell \mid \alpha &\sim& \t{Beta}(1, \alpha), &\t{for } \ell=1,\dots,K-1 \\
  \alpha &\sim& \t{Gamma(a, b)} &\t{(optional prior)}. \\
\end{eqnarray}
$$

The joint posterior of the parameters $\mathbf{\theta} = \p{\mathbf{\mu},
\mathbf{\sigma}, \mathbf{v}, \alpha}$ can be sampled from using NUTS or HMC, or
approximated via ADVI. This can be done in the various PPLs as follows. (Note that
these are excerpts from complete examples which are also linked.) 

<!-- Buttons Div for appending buttons-->
<div id="ppl-buttons" class="btn-group" role="group" aria-label="...">
</div>

{% assign ppl_array = page.ppls | split: ',' %}

{% for ppl in ppl_array %}
  {% assign ppl_lower = ppl | downcase %}

  {% if ppl_lower == "nimble" %}
<div class="ppl-code hide" id="{{ppl_lower}}">
  <p>
    ADVI, HMC, and NUTS are not supported in NIMBLE at the moment. Though,
    the model can be implemented and posterior inference can be made via
    alternative inference algorithms. See:
    <a href="https://r-nimble.org/html_manual/cha-bnp.html#sec:sb">
      this NIMBLE example.
    </a>
  </p>
</div>
  {% else %}
<div class="ppl-code hide" id="{{ppl_lower}}">
<p>
  <a href="{{page.nburl}}/dp_sb_gmm_{{ppl_lower}}.ipynb">Full {{ppl}} Example (notebook)</a>
</p>
    {% if ppl_lower == "turing" %}
      {% highlight julia linenos %}{% include_relative ppl-snippets/dp-sb-gmm/dp_sb_gmm_{{ppl_lower}}.jl %}{% endhighlight %}
    {% elsif ppl_lower == "nimble" %}
      {% highlight R linenos %}{% include_relative ppl-snippets/dp-sb-gmm/dp_sb_gmm_{{ppl_lower}}.R %}{% endhighlight %}
    {% else  %}
      {% highlight python linenos %}{% include_relative ppl-snippets/dp-sb-gmm/dp_sb_gmm_{{ppl_lower}}.py %}{% endhighlight %}
    {% endif %}
</div>
  {% endif %}
{% endfor %}

## Comparing the PPLs
TODO: Make some remarks comparing the PPLs here...

### Timings
TODO: Add a summarizing table here...



<!-- Scripts code chunk buttons -->
<script>
$(document).ready(function(){
  // PPLs to benchmark.
  var ppls = ['Turing', 'STAN', 'TFP', 'Pyro', 'Numpyro', 'NIMBLE'];

  for (ppl of ppls) {
    let ppl_lower = ppl.toLowerCase();

    // Create buttons.
    $('#ppl-buttons').append(`
      <button type="button" class="btn btn-default ${ppl_lower}">${ppl}</button>
    `);

    // Show Turing example by default.
    $("#turing").attr("class", `ppl-code show`);

    // Button callbacks. 
    $(`button.${ppl_lower}`).click(() => {
      $(".ppl-code").attr("class", `ppl-code hide`);
      $(`#${ppl_lower}`).attr("class", `ppl-code show`);
    });
  }
});
</script>

[1]: https://turing.ml/dev/tutorials/6-infinitemixturemodel/
[2]: https://www.coursera.org/learn/mixture-models
[3]: https://luiarthur.github.io/statorial/varinf/introvi/
[4]: https://arxiv.org/abs/1206.1901
[5]: http://jmlr.org/papers/v15/hoffman14a.html
